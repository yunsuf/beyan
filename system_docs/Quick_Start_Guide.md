# Document Digitization System - Quick Start Guide

> **âš¡ Get from zero to production in 30 minutes**  
> **ğŸ¯ Perfect for developers who need immediate results**

---

## ğŸš€ Quick Start (5 minutes)

**What is this?** A multi-agent LLM system that automatically processes business documents (invoices, packing lists, etc.) using Kimi-VL vision models and extracts structured data.

**Why do I need it?** Eliminates manual data entry from business documents, provides 95%+ accuracy, and scales to process thousands of documents automatically.

**How does it work?** 
- Upload documents â†’ Kimi-VL classifies document type â†’ Multi-agent system extracts data â†’ Structured JSON output
- Uses local or cloud deployment options with human-in-the-loop review

**What do I need to implement?** 
- âœ… Python environment with GPU support (recommended)
- âœ… Docker setup for containerized deployment  
- âœ… API integration for document processing
- âœ… Optional: Custom schemas for your document types

**How long will it take?** 
- â±ï¸ **5 minutes**: Basic setup and first document processed
- â±ï¸ **30 minutes**: Full production-ready deployment
- â±ï¸ **2-4 hours**: Custom document types and advanced configuration

**What are the risks?** 
- ğŸ”´ **GPU Memory**: Requires 8GB+ VRAM for optimal performance
- ğŸŸ¡ **Initial Setup**: Model downloads can take 15-30 minutes
- ğŸŸ¢ **Low Risk**: Fallback to CPU processing available

---

## ğŸ“‹ Implementation Checklist

### **Phase 1: Environment Setup (5-10 minutes)**
- [ ] âœ… **Python 3.9+ installed** (verify: `python --version`)
- [ ] âœ… **GPU drivers installed** (verify: `nvidia-smi`)
- [ ] âœ… **Docker installed** (verify: `docker --version`)
- [ ] âœ… **Clone repository and create virtual environment**
- [ ] âœ… **Install core dependencies**
- [ ] âœ… **Download Kimi-VL model** (15-30 min download time)

### **Phase 2: Basic Configuration (5-10 minutes)**
- [ ] âš™ï¸ **Create configuration file** (`config.yaml`)
- [ ] âš™ï¸ **Test with sample documents** (provided in `/sample_docs`)
- [ ] âš™ï¸ **Verify API service startup**
- [ ] âš™ï¸ **Process first document** (success = structured JSON output)

### **Phase 3: Production Deployment (10-15 minutes)**
- [ ] ğŸš€ **Docker containerization** (single command deployment)
- [ ] ğŸš€ **Health monitoring setup**
- [ ] ğŸš€ **Batch processing configuration**
- [ ] ğŸš€ **Performance validation** (process 10+ documents)

### **Phase 4: Advanced Setup (Optional - 1-2 hours)**
- [ ] ğŸ”§ **Custom document schemas**
- [ ] ğŸ”§ **Performance optimization**
- [ ] ğŸ”§ **Integration with existing systems**
- [ ] ğŸ”§ **Production monitoring and logging**

---

## âš–ï¸ Deployment Decision Matrix

| Requirement | Local GPU | Cloud GPU | CPU Only | Docker | Recommendation |
|-------------|-----------|-----------|----------|---------|----------------|
| **Performance** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­ | â­â­â­â­ | **Local GPU** for speed |
| **Cost** | â­â­â­ | â­â­ | â­â­â­â­â­ | â­â­â­â­ | **CPU Only** for budget |
| **Setup Complexity** | â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | **Docker** for simplicity |
| **Scalability** | â­â­ | â­â­â­â­â­ | â­ | â­â­â­â­ | **Cloud GPU** for scale |
| **Data Privacy** | â­â­â­â­â­ | â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | **Local GPU** for privacy |

**ğŸ¯ Recommended Quick Start Path**: Docker + Local GPU (if available) or Docker + CPU Only

---

## ğŸ› ï¸ Prerequisites & System Requirements

### **Minimum Requirements**
```bash
âœ… Python 3.9+
âœ… 8GB RAM (minimum)
âœ… 20GB free disk space
âœ… Docker & Docker Compose
```

### **Recommended Requirements**
```bash
ğŸš€ Python 3.11+
ğŸš€ NVIDIA GPU with 8GB+ VRAM
ğŸš€ 16GB+ RAM
ğŸš€ 50GB+ free disk space (for models and data)
ğŸš€ Ubuntu 20.04+ or macOS 12+
```

### **Quick Hardware Check**
```bash
# Check Python version
python --version  # Should be 3.9+

# Check available memory
free -h  # Should show 8GB+ available

# Check GPU (if applicable)
nvidia-smi  # Should show GPU with 8GB+ memory

# Check disk space
df -h  # Should show 20GB+ free space
```

---

## âš¡ 5-Minute Quick Start

### **Option A: Docker Quick Start (Recommended)**

```bash
# 1. Clone and navigate
git clone <your-repo-url>
cd beyan

# 2. Start with Docker (single command!)
docker-compose up -d

# 3. Test with sample document
curl -X POST "http://localhost:8000/documents/process" \
  -F "file=@sample_docs/2640316788_Commercial Invoice_1.pdf"

# âœ… Success! You should get structured JSON output
```

### **Option B: Local Development Setup**

```bash
# 1. Environment setup
git clone <your-repo-url>
cd beyan
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Download models (this takes 15-30 minutes)
python setup.py download_models

# 4. Start API server
uvicorn main:app --host 0.0.0.0 --port 8000

# 5. Test processing
python test_basic.py
```

---

## ğŸ“„ Configuration

### **Essential Configuration (`config.yaml`)**

```yaml
# Minimal working configuration
kimi_vl:
  model_path: "kimi-vl-base"
  device: "auto"  # Automatically detects GPU/CPU
  max_batch_size: 4

processing:
  confidence_threshold: 0.95
  max_retries: 3
  timeout_seconds: 300

storage:
  input_folder: "./sample_docs"
  output_folder: "./processed"
  
api:
  host: "0.0.0.0"
  port: 8000
```

### **Performance Configuration (Optional)**

```yaml
# Advanced performance settings
kimi_vl:
  model_path: "kimi-vl-base"
  device: "cuda:0"  # Specific GPU
  max_batch_size: 8  # Higher for better GPU utilization
  precision: "fp16"  # Faster inference
  enable_cache: true

processing:
  parallel_workers: 4
  queue_size: 100
  batch_processing: true
```

---

## ğŸ§ª Testing & Validation

### **1. Basic Functionality Test**

```python
# quick_test.py - Run this to verify everything works
import requests
import json
from pathlib import Path

def test_system():
    """Test basic document processing functionality"""
    
    # Test API health
    response = requests.get("http://localhost:8000/health")
    assert response.status_code == 200, "âŒ API not responding"
    print("âœ… API health check passed")
    
    # Test document processing
    test_file = "sample_docs/2640316788_Commercial Invoice_1.pdf"
    if Path(test_file).exists():
        with open(test_file, 'rb') as f:
            files = {'file': f}
            response = requests.post("http://localhost:8000/documents/process", files=files)
        
        if response.status_code == 200:
            result = response.json()
            print(f"âœ… Document processed successfully")
            print(f"   Type: {result.get('document_type', 'unknown')}")
            print(f"   Confidence: {result.get('overall_confidence', 0):.2f}")
            return True
        else:
            print(f"âŒ Processing failed: {response.text}")
            return False
    else:
        print(f"âŒ Test file not found: {test_file}")
        return False

if __name__ == "__main__":
    success = test_system()
    if success:
        print("\nğŸ‰ System is working correctly!")
        print("Next steps: Process your own documents or customize schemas")
    else:
        print("\nâš ï¸ System test failed. Check the troubleshooting section below.")
```

Run the test:
```bash
python quick_test.py
```

### **2. Batch Processing Test**

```python
# batch_test.py - Test processing multiple documents
import os
import requests
import time
from pathlib import Path

def batch_process_test(folder_path="sample_docs"):
    """Test batch processing of all sample documents"""
    
    folder = Path(folder_path)
    supported_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.tiff']
    
    results = []
    start_time = time.time()
    
    for file_path in folder.iterdir():
        if file_path.suffix.lower() in supported_extensions:
            print(f"Processing: {file_path.name}")
            
            with open(file_path, 'rb') as f:
                files = {'file': f}
                response = requests.post("http://localhost:8000/documents/process", files=files)
            
            if response.status_code == 200:
                result = response.json()
                results.append(result)
                print(f"  âœ… {result['document_type']} (confidence: {result['overall_confidence']:.2f})")
            else:
                print(f"  âŒ Error: {response.status_code}")
    
    end_time = time.time()
    
    # Summary
    total_docs = len(results)
    avg_confidence = sum(r['overall_confidence'] for r in results) / total_docs if results else 0
    processing_time = end_time - start_time
    
    print(f"\nğŸ“Š Batch Processing Results:")
    print(f"   Documents processed: {total_docs}")
    print(f"   Average confidence: {avg_confidence:.2f}")
    print(f"   Total time: {processing_time:.1f} seconds")
    print(f"   Average per document: {processing_time/total_docs:.1f} seconds")

if __name__ == "__main__":
    batch_process_test()
```

---

## ğŸ³ Docker Deployment

### **Quick Docker Start**

```bash
# Start all services with one command
docker-compose up -d

# Check status
docker-compose ps

# View logs
docker-compose logs -f kimi-vl-processor
```

### **Manual Docker Build (if needed)**

```bash
# Build image
docker build -t kimi-vl-processor .

# Run with GPU support
docker run --gpus all -p 8000:8000 \
  -v $(pwd)/sample_docs:/app/data \
  -v $(pwd)/processed:/app/output \
  kimi-vl-processor

# Run CPU-only
docker run -p 8000:8000 \
  -v $(pwd)/sample_docs:/app/data \
  -v $(pwd)/processed:/app/output \
  kimi-vl-processor
```

---

## ğŸ”§ Troubleshooting

### **ğŸš¨ Common Issues & Quick Fixes**

| Issue | Symptoms | Quick Fix | Prevention |
|-------|----------|-----------|------------|
| **CUDA Out of Memory** | `RuntimeError: CUDA out of memory` | Reduce `max_batch_size: 1` in config | Monitor GPU memory with `nvidia-smi` |
| **Model Download Failed** | `ConnectionError` or `TimeoutError` | Re-run `python setup.py download_models` | Use stable internet connection |
| **API Not Starting** | `Connection refused` on port 8000 | Check if port is free: `lsof -i :8000` | Use different port in config |
| **Slow Processing** | >30 seconds per document | Enable GPU or increase batch_size | Check GPU utilization |
| **Permission Errors** | `PermissionError` on file operations | Check folder permissions: `chmod 755` | Run with proper user permissions |

### **ğŸ” Diagnostic Commands**

```bash
# Check system health
curl "http://localhost:8000/health"

# Check GPU availability
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"

# Check model loading
python -c "from kimi_vl import KimiVLService; print('Model loaded successfully')"

# Check disk space
df -h | grep -E '(Available|filesystem)'

# Check memory usage
free -m

# Check running processes
ps aux | grep python
```

### **ğŸš¨ Error Resolution Guide**

#### **Error: "ModuleNotFoundError: No module named 'kimi_vl'"**
```bash
# Solution: Install requirements properly
pip install -r requirements.txt
pip install --upgrade kimi-vl

# Verify installation
python -c "import kimi_vl; print('âœ… Kimi-VL installed')"
```

#### **Error: "RuntimeError: CUDA out of memory"**
```bash
# Solution 1: Reduce batch size
# Edit config.yaml:
max_batch_size: 1

# Solution 2: Clear GPU cache
python -c "import torch; torch.cuda.empty_cache()"

# Solution 3: Use CPU processing
# Edit config.yaml:
device: "cpu"
```

#### **Error: "Port 8000 already in use"**
```bash
# Find what's using the port
lsof -i :8000

# Kill the process (replace PID)
kill -9 <PID>

# Or use different port
uvicorn main:app --port 8001
```

### **ğŸ¥ Health Monitoring**

```python
# health_check.py - Comprehensive system health check
import requests
import psutil
import torch
import time

def comprehensive_health_check():
    """Perform complete system health check"""
    
    print("ğŸ¥ System Health Check")
    print("=" * 50)
    
    # System resources
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage('/')
    
    print(f"ğŸ’¾ Memory: {memory.available / (1024**3):.1f}GB available ({memory.percent}% used)")
    print(f"ğŸ’½ Disk: {disk.free / (1024**3):.1f}GB available ({disk.percent}% used)")
    
    # GPU check
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        gpu_allocated = torch.cuda.memory_allocated(0) / (1024**3)
        print(f"ğŸ® GPU: {gpu_memory:.1f}GB total, {gpu_allocated:.1f}GB allocated")
    else:
        print("ğŸ® GPU: Not available (using CPU)")
    
    # API health
    try:
        response = requests.get("http://localhost:8000/health", timeout=5)
        if response.status_code == 200:
            print("ğŸŒ API: âœ… Healthy")
        else:
            print(f"ğŸŒ API: âŒ Error {response.status_code}")
    except requests.exceptions.RequestException as e:
        print(f"ğŸŒ API: âŒ Not responding ({e})")
    
    # Performance test
    start_time = time.time()
    try:
        response = requests.post(
            "http://localhost:8000/documents/process",
            files={'file': open('sample_docs/2640316788_Commercial Invoice_1.pdf', 'rb')},
            timeout=30
        )
        processing_time = time.time() - start_time
        if response.status_code == 200:
            print(f"âš¡ Performance: âœ… Processed test document in {processing_time:.1f}s")
        else:
            print(f"âš¡ Performance: âŒ Processing failed")
    except Exception as e:
        print(f"âš¡ Performance: âŒ Test failed ({e})")
    
    print("=" * 50)

if __name__ == "__main__":
    comprehensive_health_check()
```

---

## ğŸ¯ Success Criteria Checklist

**âœ… Minimum Viable Setup (5 minutes)**
- [ ] API responds to health check
- [ ] Successfully processes one sample document
- [ ] Returns structured JSON output
- [ ] Confidence score > 0.8

**âœ… Production Ready (30 minutes)**
- [ ] Processes all sample documents successfully
- [ ] Average processing time < 10 seconds per document
- [ ] Docker deployment working
- [ ] Batch processing functional
- [ ] Health monitoring active

**âœ… Advanced Configuration (1-2 hours)**
- [ ] Custom document schemas implemented
- [ ] Performance optimized for your hardware
- [ ] Integration with existing systems
- [ ] Monitoring and logging configured

---

## ğŸš€ Next Steps

### **Immediate Actions (After Quick Start)**
1. **Process Your Documents**: Replace sample documents with your real business documents
2. **Customize Schemas**: Add your specific document types and fields
3. **Performance Tuning**: Optimize for your hardware and volume requirements
4. **Integration Planning**: Plan integration with your existing business systems

### **Scaling Considerations**
- **Volume**: For >1000 documents/day, consider cloud deployment
- **Accuracy**: For specialized documents, plan custom model fine-tuning  
- **Integration**: For enterprise use, implement proper authentication and monitoring
- **Compliance**: For regulated industries, ensure data handling compliance

### **Advanced Features to Explore**
- **n8n Visual Workflow Automation** - Replace custom orchestration with visual workflows
- Multi-agent human-in-the-loop review system
- Custom document type training
- Advanced OCR preprocessing
- Real-time processing workflows
- Analytics and reporting dashboards

---

## ğŸ“š Additional Resources

### **Documentation**
- ğŸ“– **[System Design Guide](./system_design.md)** - Detailed architecture and design decisions
- ğŸ”§ **[Technical Implementation Guide](./Kimi-VL_Technical_Implementation_Guide.md)** - Advanced configuration and customization
- ğŸ¤– **[Multi-Agent Framework Guide](../ai-docs/)** - LLM agent implementation details
- ğŸ”„ **[n8n Integration Guide](../ai-docs/n8n-integration-guide.md)** - Visual workflow automation setup

### **Support & Community**
- ğŸ› **Issues**: [GitHub Issues](https://github.com/your-repo/issues)
- ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/your-repo/discussions)  
- ğŸ“§ **Email**: support@yourcompany.com
- ğŸ“– **Wiki**: [Project Wiki](https://github.com/your-repo/wiki)

---

**âš¡ Quick Start Complete!** You should now have a working document digitization system. Time to process some real documents! ğŸ‰ 